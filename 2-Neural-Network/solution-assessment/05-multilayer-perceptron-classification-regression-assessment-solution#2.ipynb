{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the MPL course!! Hopefully, you learned some valuable skills along the way and had fun doing it. Now it is time to put those skills to the test. In this assessment we have two problems:  First, you will train a new model capable of classifying the $10$ different classes all included in the database known as [_CIFAR-10_](https://www.cs.toronto.edu/~kriz/cifar.html). You will need to get the model to a validation to pass the assessment, although we challenge you to do even better if you can. You will have to use the skills you learned in the previous exercises.  Second, you will solve [_Kaggle's Titanic_](https://www.kaggle.com/competitions/titanic) challenge. The task consists of processing the data, generating the model, and doing a Deploy. Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem #1: `CIFAR-10 Classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "To start studying neural networks, it is necessary to understand and be able to apply the most basic concepts of recognition to your model. A good way to test and train your model is to use the CIFAR-10 database which consists of a set of $60,000$ images for $10$ different classes (planes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks) which are identified from $0$ to $9$ respectively. With that in mind, you will train your model on top of this base, making it possible to correctly recognize each element of these classes.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    " <img src=\"../images/cifar-10.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of images from the CIFAR-10 database\n",
    "\n",
    "The images have dimensions of $32$x$32$ so remember to adjust your image when making a prediction with the model, for each class there are $6,000$ images related to its type so you must best adjust the number of training epochs as well as the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "\n",
    "1. Create an MLP Neural Network.\n",
    "\n",
    "2. Import the CIFAR-10 database from the torchvision library.\n",
    "\n",
    "3. Train your model so that it correctly performs predictions, paying attention to the input parameters.\n",
    "\n",
    "4. Perform the prediction with some image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem #2: `Titanic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in 1502 out of 2224 passenger and crew deaths.\n",
    "\n",
    "While some element of luck was involved in surviving, some groups were more likely to survive than others.\n",
    "\n",
    "In this problem, we ask you to build a predictive model that answers the question: _What sorts of people were more likely to survive?_ using passenger data (i.e., name, age, gender, socio-economic class, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, you will gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv`, and the other is titled `test.csv`.\n",
    "\n",
    "`train.csv` will contain the details of a subset of the passengers on board ($891$ to be exact) and, importantly, will reveal whether they survived, also known as the _ground truth_.\n",
    "\n",
    "The `test.csv` dataset contains similar information but does not disclose the _ground truth_ for each passenger. It is your job to predict these outcomes.\n",
    "\n",
    "Using the patterns in the `train.csv` data, predict whether the other $418$ passengers on board (found in `test.csv`) survived.\n",
    "\n",
    "Check out the *Data* tab to explore the datasets even further. Once you feel you have created a competitive model, please submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Variable | Definition | \tKey |\n",
    "|----------|------------|-------|\n",
    "| survival | Survival\t| 0 = No, 1 = Yes|\n",
    "| pclass   | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex\t   | Sex\t| |\n",
    "| Age\t   | Age in years\t| |\n",
    "| sibsp\t   | # of siblings / spouses aboard the Titanic\t| |\n",
    "| parch\t   | # of parents / children aboard the Titanic\t| |\n",
    "| ticket   | Ticket number\t| |\n",
    "| fare\t   | Passenger fare\t| |\n",
    "| cabin\t   | Cabin number\t| |\n",
    "| embarked | Port of Embarkation |\tC = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "#### Variable Notes\n",
    "\n",
    "**pclass**: A proxy for socio-economic status (SES)\n",
    "\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "---\n",
    "\n",
    "**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "----\n",
    "\n",
    "**sibsp**: The dataset defines family relations in this way...\n",
    "\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "----\n",
    "**parch**: The dataset defines family relations in this way...\n",
    "\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the MPL course!! Hopefully, you learned some valuable skills along the way and had fun doing it. Now it is time to put those skills to the test. In this assessment, you will train a new model capable of classifying the 10 different classes all included in the database known as [_CIFAR-10_](https://www.cs.toronto.edu/~kriz/cifar.html). You will need to get the model to a validation to pass the assessment, although we challenge you to do even better if you can. You will have to use the skills you learned in the previous exercises. Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ☆ Solution Problem #2 - Titanic ☆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Import Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Loading The Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../datasets/titanic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"{path}/train.csv\", sep=\",\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_missing(train_data):    \n",
    "    for col in train_data.columns.tolist():          \n",
    "        print('{} missing data: {}'.format(col, train_data[col].isnull().sum()))\n",
    "    print('\\n')\n",
    "    \n",
    "display_missing(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Loading The Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f\"{path}/test.csv\", sep=\",\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Treating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove name and tickets\n",
    "\n",
    "2. map gender to:\n",
    "\n",
    "Male: 0<br>\n",
    "Female: 1\n",
    "\n",
    "3. Map Embarked to:\n",
    "\n",
    "C: 0\n",
    "Q: 1\n",
    "S: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_sex = {'male': 0, 'female': 1}\n",
    "train_data = train_data.replace({'Sex': mapping_sex})\n",
    "test_data = test_data.replace({'Sex': mapping_sex})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_embarked = {'C': 0, 'Q': 1, 'S': 2}\n",
    "train_data = train_data.replace({'Embarked': mapping_embarked})\n",
    "test_data = test_data.replace({'Embarked': mapping_embarked})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fill in the data that has no age. For this, let's try to find the correlation between age and other attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_corr = train_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "train_data_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "train_data_corr[train_data_corr['Feature 1'] == 'Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there is a high correlation between age and Pclass, so we can use Pclass to try to fill in age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we have a suspicion that gender can greatly influence the average if we use social class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_by_pclass_sex = train_data.groupby(['Sex', 'Pclass']).median()['Age']\n",
    "\n",
    "for pclass in range(1, 4):\n",
    "    # 0 - male, 1 - female\n",
    "    for sex in [1, 0]:\n",
    "        print('Average age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\n",
    "print('Average age of all: {}'.format(train_data['Age'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average age differs greatly between Pclass and gender. So we can use these two pieces of information to fill in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Age'] = train_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to treat the missing cabins. The position of the place on the ship must influence survival (whoever crashed first may have died from the crash, for example).\n",
    "\n",
    "And we know that the organization is usually by social class. So let's try to relate the first letter of the cabin (which should be the sector) with the social class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting M for missing on missing data\n",
    "train_data['Cabin'] = train_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "test_data['Cabin'] = test_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Cabin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to relate the lyrics to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cabin_corr = train_data.groupby(['Cabin', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n",
    "                                                                        'Fare', 'Embarked', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cabin_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cabin = {'A': 0, 'B': 0, 'C': 0, 'D': 1, 'E': 1, 'F': 2, 'G': 2, 'T': 2, 'M': 3}\n",
    "train_data = train_data.replace({'Cabin': mapping_cabin})\n",
    "test_data = test_data.replace({'Cabin': mapping_cabin})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missing(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to treat the embarked, which has 2 missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_corr = train_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "train_data_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "train_data_corr[train_data_corr['Feature 1'] == 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[[col for col in train_data.columns if col not in ['Name', 'Ticket']]]\n",
    "test_data = test_data[[col for col in test_data.columns if col not in ['Name', 'Ticket']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    plt.figure(figsize=[16, 12])\n",
    "    train_data[col].plot()\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Import Library Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_datax = train_data[[col for col in train_data.columns if col not in ['Survived']]]\n",
    "mlp_train_datay = train_data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_x_train = mlp_train_datax.values.astype(np.float32)\n",
    "mlp_y_train = mlp_train_datay.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_x_test = test_data.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗ Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mlp_x_train = scaler.fit_transform(mlp_x_train)\n",
    "mlp_x_test = scaler.transform(mlp_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗  Preparation of training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "mlp_y_train = np.eye(num_classes)[mlp_y_train.reshape(-1).astype(int)]\n",
    "mlp_y_train = torch.from_numpy(mlp_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = mlp_x_train.shape[1]\n",
    "hidden_size = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model compilation and optimizer definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(mlp_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⊗  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "mlp_x_train = torch.from_numpy(mlp_x_train)\n",
    "mlp_dataset = torch.utils.data.TensorDataset(mlp_x_train, mlp_y_train)\n",
    "mlp_dataloader = torch.utils.data.DataLoader(mlp_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train()\n",
    "mlp_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in mlp_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(inputs)\n",
    "        loss = criterion(outputs, torch.argmax(targets, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(mlp_dataloader)\n",
    "    mlp_loss_history.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    mlp_y_pred = mlp_model(mlp_x_test)\n",
    "    mlp_predictions = torch.argmax(mlp_y_pred, dim=1)\n",
    "\n",
    "mlp_predictions = mlp_predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model.state_dict(), f\"{path}/mlp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(mlp_loss_history, color='blue')\n",
    "plt.title('Model loss', fontsize=20)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': mlp_predictions})\n",
    "output.to_csv(f\"{path}/mlp_submission.csv\", index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FolLqE_1bID"
   },
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXiePpX31bID"
   },
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMB90WpX1bID",
    "outputId": "1df63f58-1161-495d-c481-2713ff31a7fa"
   },
   "outputs": [],
   "source": [
    "#import IPython\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Temporary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before finished the assessment, please execute the following cell to clear up the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../models/handwritten-model.pt ../datasets/cifar-10-python.tar.gz  ../datasets/cifar-10-batches-py ../datasets/cifar-10-batches-py ../datasets/MNIST  cifar-10*  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
