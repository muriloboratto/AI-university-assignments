{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21d5059-73b7-456c-82ea-f1a605a0ad14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# High-performance computing applied in AI solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37b173-ed95-46f5-bdf0-0bdf3016caed",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "High-performance computing, known as HPC, is a field of modern computing whose goal is to solve computational problems of high complexity and large volumes of data by operating by dividing complex problems into smaller parts that are processed simultaneously by several processors, accelerating the resolution time. HPC enables scientists, engineers, and researchers to perform highly detailed simulations, massive data analysis, and precise modeling that would be impractical or unachievable using conventional systems.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "HPC systems are designed to handle large volumes of data and perform intensive calculations in a fraction of the time it would take on conventional computers. An HPC comprises one or several supercomputers of interconnected high-performance processors, large amounts of memory, and fast storage to handle intensive workloads.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure01_ponte_vecchio.jpg\" style=\"width: 500px;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762f150-1969-42e6-b8ce-dbb31fbebb96",
   "metadata": {},
   "source": [
    "## ⊗ **Why use parallel computing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0cc88-a7de-4cb6-8b31-40f3d1de4692",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>   \n",
    "Parallel computing is widely used in many areas such as scientific simulations, graphics rendering, big data analysis, machine learning, artificial intelligence, image processing and many more. There are several approaches to implementing parallel computing, of which we can include data parallelism, task parallelism, instruction parallelism, bit-level parallelism, thread-level parallelism, among others.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure02_parallel_computing.png\" style=\"width: 1000px;\">\n",
    "</div>\n",
    "\n",
    "<p style='text-align: justify;'>   \n",
    "One of the main gains parallel computing provides is the remarkable performance acceleration. You can get more work done in less time by running multiple tasks simultaneously. This aspect is particularly advantageous for solving complex problems that often involve intensive calculations or the analysis of vast data sets.\n",
    "Parallel computing is essential for dealing with the growing data generated in our digital world. In machine learning and artificial intelligence, training complex models in parallel is critical for creating effective AI systems in areas such as pattern recognition, natural language processing, and computer vision.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "HPC and parallel computing can be used in several scenarios. Let's meet some of them below.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a215da7-040a-4598-8772-bf1e990001fa",
   "metadata": {},
   "source": [
    "## ⊗ **HPC applied in AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22840e1e-0850-4e66-90db-d1f795bb6ea6",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "HPC plays a key role when it comes to applications using Artificial Intelligence, since a large computational power is needed to be able to train increasingly complex AI models and perform analyzes on massive data sets.\n",
    "</p>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure03_aurora_supercomputing.jpg\" style=\"width: 500px;\">\n",
    "</div>\n",
    "    \n",
    "<p style='text-align: justify;'> \n",
    "A notable example is Intel's Aurora supercomputer, which plays a key role in research areas as diverse as neuroscience, aerospace simulation, universe exploration, and artificial intelligence. These surveys require an extremely high processing capacity,\n",
    "making the application of structures such as HPC essential. Conducting research in this direction requires the use of computational algorithms capable of dealing with large volumes of data and that also have resources for implementing artificial intelligence solutions. For example, in neuroscience research,\n",
    "Aurora can help simulate complex neural networks and analyze brain data at scale, leading to advances in understanding neurological disorders and developing more effective treatments.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "In aerospace simulations and exploration of the universe, Aurora allows the modeling of complex phenomena, such as the behavior of planetary systems and aircraft flight dynamics, contributing to space exploration and the development of more advanced technologies. In summary, the intersection between HPC and life science research, space and AI drives scientific discoveries and technological advances that have the potential to transform our lives and our understanding of the world around us.\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d0c4c-17a0-481f-928b-cc669f30c1ec",
   "metadata": {},
   "source": [
    "## ⊗ **HPC uses cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833036e-082b-4f1d-88f7-05ffa4221acd",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "HPC is often used in fields where processing requirements are extraordinarily high and exceed the capabilities of conventional computer systems. Here are some examples of HPC use cases:\n",
    "</p>\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/figure04_hpc_applications.png\" style=\"width: 500px;\">\n",
    "</div>\n",
    "\n",
    "* **Machine learning and artificial intelligence:** training complex machine learning models requires a lot of computing power. HPC allows you to train models faster and handle larger datasets, resulting in advances in AI, pattern recognition and data analysis;\n",
    "\n",
    "* **Biomedical research:** HPC accelerates the virtual screening of molecules, assessing how they interact with target proteins. This streamlines the drug discovery process, saving time and resources;\n",
    "\n",
    "* **Aerodynamics and flight simulation:** the aerospace industry uses HPC to simulate the behavior of aircraft,improve wing design, optimize fuel efficiency and study aerodynamics;\n",
    "\n",
    "* **Exploration of natural resources and petroleum:** the simulation of oil and gas reservoirs, as well as the exploration of mineral resources, require complex models and intensive calculations. HPC helps make informed decisions about locating and exploiting these resources;\n",
    "\n",
    "* **Particle physics:** particle physics research requires HPC to analyze data generated by particle accelerators such as the LHC (Large Hadron Collider);\n",
    "\n",
    "* **Scientific research and simulations:** HPC allows the modeling of natural phenomena and processes that would be almost impossible to observe experimentally. For example, simulating particle interactions in a particle accelerator or simulating long-term weather processes.\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "These are just a few examples of the many use cases for HPC. In general, it plays a crucial role in areas that need advanced computational capacity to solve complex problems, often driving innovation and scientific and technological progress.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c549e24-b460-4f22-8cea-0b32287d71d4",
   "metadata": {},
   "source": [
    "## **HPC as solutions for AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4518c-36e7-4d4b-a3d1-aa1a62981587",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "When we want to deal with a large volume of information in artificial intelligence applications, aiming to substantially reduce the time required to solve the problem, it is essential that specific software tools are implemented for this purpose. Let's now explore two of the most prominent libraries: <a href='https://www.tensorflow.org/api_docs' target='_blank'><em>Tensorflow</em></a> and <a href='https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html' target='_blank'><em>Pytorch</em></a>. These tools play a central role in creating and training AI models in processing-intensive environments, let's get to know each one of them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72b0a4-8133-45ea-9acf-a5df54845269",
   "metadata": {},
   "source": [
    "### ⊗ **Tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342a08-64df-4712-9943-25d9a01f3173",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "<em>Tensorfdlow</em> is an open-source library focused on high-performance numerical computing, especially suitable for training and deploying machine learning and deep learning models. It is used in various applications, from computer vision to natural language processing.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "In our algorithms, we will be using a package called <a href='https://www.tensorflow.org/guide/keras?hl=pt-br' target='_blank'><em>Keras</em></a>, which is nothing more than a high-level API for building and training neural networks. Its main feature is to simplify and streamline the development of deep learning models.\n",
    "</p>  \n",
    "<p style='text-align: justify;'>\n",
    "Let's see how we can access and utilize our CPU to train a simple neural network using TensorFlow.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d194d0-9328-435f-abaf-4746de85e2c7",
   "metadata": {},
   "source": [
    "#### **Checking the environmental availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27db308b-c108-489d-aef4-17dcc9ead7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Configure GPU memory allocation dynamically\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    # Display information about available GPU\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu.name}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93378488-1958-4169-8851-9634c3fa9246",
   "metadata": {},
   "source": [
    "####  **Creating a neural network with Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c513f92-82c8-4db9-8d0f-86f0ebceadc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.7790 - accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7787 - accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7783 - accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7780 - accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7776 - accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7773 - accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7769 - accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7766 - accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7762 - accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7759 - accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7756 - accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7752 - accuracy: 0.7500\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7749 - accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7745 - accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7742 - accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7738 - accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7735 - accuracy: 0.7500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7732 - accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7728 - accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7725 - accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.7722 - accuracy: 0.7500\n",
      "Model accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Set the training data\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "\n",
    "# create the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_dim=2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(X_train, y_train, epochs=20)\n",
    "\n",
    "# rate the model\n",
    "accuracy = model.evaluate(X_train, y_train)[1]\n",
    "print(f'Model accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0713a-ae86-40ff-b8b5-67c6584b643d",
   "metadata": {},
   "source": [
    "### ⊗ **Pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bcf97-c2cb-486f-8f26-def566ae4b9b",
   "metadata": {},
   "source": [
    "<em>Pytorch</em> is an open source machine learning library known for its flexibility and ease of use, making it a popular choice among deep learning researchers and developers. Let's see the same code we did, but now with Pytorch.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59402a2-4733-47b2-ae15-df9a1a870e07",
   "metadata": {},
   "source": [
    "#### **Checking the environmental availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c189b174-bee5-4270-b3cf-3ed4d90411df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Display information about available GPUs\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2972da-3852-4a28-b8f4-34a593cd5bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  **Creating a neural network with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baefb205-d9ab-4d27-8ed0-07e5ae3d4582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define training data as tensors\n",
    "X_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_train = torch.tensor([0, 1, 1, 0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Moving model and data to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# rate the model\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_train)\n",
    "    predicted = (predicted > 0.5).float()\n",
    "    accuracy = (predicted == y_train).sum().item() / len(y_train)\n",
    "    print(f'Model accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8f158-6d94-447b-844e-71e3307be024",
   "metadata": {
    "id": "BMvo8TG2WWZE"
   },
   "source": [
    "##  ☆ Challenge: Zoo breakout!☆ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bfc0f-feac-431e-8365-7ac4dfbb0135",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "    Recently, an unexpected incident occurred at the local zoo, <b>Orange Grove Zoo</b>: all the animals escaped from their enclosures and are now roaming freely. To deal with this situation, we need your help locating and classifying the escaped animals, distinguishing each animal class, and identifying possible vehicles in the same environment.\n",
    "</p>\n",
    "<p style='text-align: justify;'> \n",
    "You have been assigned as the person responsible for developing a computer vision system capable of identifying and classifying the escaped animals and identifying the presence of vehicles in the images. We will use the CIFAR-10 dataset and the TensorFlow library to train a deep-learning model for this challenge.\n",
    "</p>\n",
    "CIFAR-10  datasets comprehensively collect $32$x$32$ pixel images grouped into $10$ distinct classes.\n",
    "\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html): CIFAR-10 consists of $60,000$ images, each belonging to one of the ten classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. This dataset offers a diverse set of images representing everyday objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af1c08-6dbd-4edc-bd08-10d0b343e9c8",
   "metadata": {
    "id": "X9pJ2tDOCaN6"
   },
   "source": [
    "a) **Create** deep neural network model utilizing the Tensorflow, and Pytorch library for the classification of animals, and vehicles on CPU using CIFAR-10 dataset,\n",
    "\n",
    "b) **Measure** execution time for the algorithm CIFAR-10 on CPU environment,\n",
    "\n",
    "c) **Justify** why it is more interesting to use the tools (Tensorflow, and Pytorch) in conjunction on GPU or CPU environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e44080-2c4a-41ce-920f-0aa37619197f",
   "metadata": {},
   "source": [
    "### ☆ Solution  CIFAR-10 using TensorFlow on CPU environment☆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506cdef-8879-4b08-9ed9-aa3fdd556b72",
   "metadata": {},
   "source": [
    "#### ⊗ Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60afa1a-bbc4-4f9e-b003-6ad6bbbd5c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec2fb0-9ab0-4878-bc23-bea3898fb2f4",
   "metadata": {},
   "source": [
    "#### ⊗ Verify the devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5984875c-6e4b-43dd-87ad-d9700ba58f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU available\n"
     ]
    }
   ],
   "source": [
    "# Checking if CPU is available\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "if not cpus:\n",
    "    raise RuntimeError(\"No CPU available.\")\n",
    "else:\n",
    "    print(\"CPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2bb9-4e6b-4183-925e-7917af3ac20f",
   "metadata": {},
   "source": [
    "#### ⊗ Downloading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324686a8-84d0-4bae-bbce-cd7c3dc35575",
   "metadata": {},
   "source": [
    "Now we need to download the CIFAR-10 dataset to be able to make predictions. CIFAR-10 is a dataset of labeled images, meaning that each image to be loaded already has a known label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6343c6-cab6-462d-a960-3c8b035c2e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79da6c5-c55a-4e9d-9282-cf1329b3ea3a",
   "metadata": {},
   "source": [
    "#### ⊗ Normalizing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb1ad8-c68d-4c42-bebf-126bf4cff917",
   "metadata": {},
   "source": [
    "After downloading the entire set of images, we need to normalize them so that we can use them in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84898485-0f1d-481b-9b31-5c45d3a33fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalizing pixel values to the [0, 1] range\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468a2db-ad64-46bc-981c-c7c7d0aca390",
   "metadata": {},
   "source": [
    "#### ⊗ Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292b711-e7b0-4d50-bcc0-c1d55801006c",
   "metadata": {},
   "source": [
    "Now it is necessary to create the model for our neural network, notice that this step becomes extremely simple using the power of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42537bc-3868-4908-af97-89defdd0ce2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24549d21-f0a7-4017-9f08-85cb30166d26",
   "metadata": {},
   "source": [
    "#### ⊗ Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb022c2f-ab93-4a20-bf90-0f56aac64049",
   "metadata": {},
   "source": [
    "After creating the model, it needs to be compiled, just as we did in the first example with CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4293b7-766c-4daf-be80-60c6c3a1227e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1754fde-9660-4613-85f3-7ee67438dcdd",
   "metadata": {},
   "source": [
    "#### ⊗ Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf06f61-3f3a-4ab5-aaa2-8faef948639d",
   "metadata": {},
   "source": [
    "Now it is important to define the training function of the model, which it will use to train using the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09202cf7-a378-4f39-a700-61727fa4ab87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train the model and measure time with progress\n",
    "def train_model(device, train_images, train_labels):\n",
    "    with tf.device(device):\n",
    "        start_time = time.time()\n",
    "        history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels), verbose=1)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    return history, end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb4b04-0745-4541-aab9-65c5d91a725c",
   "metadata": {},
   "source": [
    "#### ⊗ Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5f17e-6482-44ec-9c3f-f5b965ee104e",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> The next step is to perform the model training. Note that in the step below, we will use the CPU to train the model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d32bb23-5316-45d9-8b1d-cb1eb3949c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0976 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 2.3028 - accuracy: 0.0983 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0967 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0980 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0987 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.3028 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0989 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3027 - accuracy: 0.1001 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.3028 - accuracy: 0.0979 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.3028 - accuracy: 0.1003 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "CPU Training time: 252.98 seconds or (4.22 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the CPU \n",
    "cpu_history, cpu_time = train_model('/CPU:0', train_images, train_labels)\n",
    "print(f\"\\nCPU Training time: {cpu_time:.2f} seconds or ({cpu_time / 60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb55e02-f26e-47b4-8ecc-4aa6401db64d",
   "metadata": {},
   "source": [
    "### Comments of the results using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a417b6-adef-4ace-a1ea-fa408941e32d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style='text-align: justify;'> \n",
    "We observed that the training process of our neural network took about <b> 253 seconds (4.22 minutes)</b> for a dataset being trained for 10 epochs. This represents a significantly reduced number of training epochs in computational terms, especially when compared to larger data sets. Therefore, carrying out more extensive training on the CPU becomes impractical, making it necessary to use more robust computing resources, such as a graphics processing unit (GPU).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbaaec8-72a9-4727-a741-5a80b636ed81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hardware and software setup used in the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed267db-76b5-4263-b3c7-da17779e761e",
   "metadata": {},
   "source": [
    "The computational system used in all experiments has the operational system Red Hat Enterprise Linux, and all experiments performed in this work were run on an environment offering GPU node, where this node contains one Intel(R) i7-1165G7, CPU @4.70 GHz, 64 GB RAM, and 1 Intel Tiger Lake Gen12.  This architecture provides 16 streaming multiprocessors with 8 GB HBM2 memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb535f-68bb-4703-bb0a-f74372dbd09c",
   "metadata": {
    "id": "8_Ch1ZPO1bIC"
   },
   "source": [
    "### ☆ Solution  CIFAR-10 using Pytorch on CPU environment☆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81175015-e630-416f-9d36-2a30cc9830da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ⊗ Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a211df6a-b172-45b0-8375-8a1c24add541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b915680-f10b-4f3a-a799-7c09a243a7f8",
   "metadata": {},
   "source": [
    "#### ⊗ Verify the devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c257254-96bc-4592-b247-e2f8b2f67009",
   "metadata": {},
   "source": [
    "It is very important, before trying to execute anything on any device, to verify if it is available and if pytorch can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd569a7-3acb-4caa-86d1-f2ead2ea05b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29138cf0-ba87-4c6a-8c88-111dc159b833",
   "metadata": {},
   "source": [
    "#### ⊗ Transformations to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915d780-1d61-4f6a-9ec4-154e64ac4896",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "    As part of the data preparation process, we create a <b>transforms</b> object to apply specific transformations to the data. These transformations are commonly used in training datasets to enhance data diversity and ready images for utilization in a deep learning model, such as a Convolutional Neural Network (CNN).\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210c3993-fedc-4018-88b8-48dc30df64da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d6960-4abe-48b3-9167-3bd9454e037d",
   "metadata": {},
   "source": [
    "#### ⊗ Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc889d5f-3ace-4c72-bf7c-5ec768c8bcc1",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "Following that, download the CIFAR-10 dataset and load it into the code. Define the neural network as we have done in previous notebooks, and remember to move this network instance to the previously defined device.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89db0f4-fd04-4b3e-a59e-79220d531953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [03:10<00:00, 896061.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0375d-d0f8-41f0-9530-5459d2c125f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ⊗ Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c9319-c3a3-4895-9c60-58ae53e2f6ff",
   "metadata": {},
   "source": [
    "Now it is necessary to create the model for our neural network using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9295417d-ad2f-421c-b053-16f5b4db5b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb622a0f-9d02-4d7c-aaf5-572e4529ebed",
   "metadata": {},
   "source": [
    "#### ⊗ Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4fa56-591a-418b-b52e-ff58d543565c",
   "metadata": {},
   "source": [
    "Now the training of our neural network will be carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be67ba50-0e8d-435f-ada7-b0a98b91aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4932993039908007\n",
      "Epoch 2, Loss: 0.47943620257975195\n",
      "Epoch 3, Loss: 0.47039098767063503\n",
      "Epoch 4, Loss: 0.4571740362802735\n",
      "Epoch 5, Loss: 0.4399987073505626\n",
      "Epoch 6, Loss: 0.42618661684453335\n",
      "Epoch 7, Loss: 0.4213465188470338\n",
      "Epoch 8, Loss: 0.41050467756398196\n",
      "Epoch 9, Loss: 0.40083302690854766\n",
      "Epoch 10, Loss: 0.3957774586918409\n",
      "\n",
      "CPU Training time: 811.17 seconds or (13.52 minutes)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "cpu_start_time = time.time()\n",
    "\n",
    "for epoch in range(10):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "cpu_end_time = time.time()\n",
    "\n",
    "cpu_time = cpu_end_time - cpu_start_time\n",
    "\n",
    "print(f\"\\nCPU Training time: {cpu_time:.2f} seconds or ({cpu_time / 60:.2f} minutes)\")\n",
    "\n",
    "torch.save(net.state_dict(), 'cifar10_cpu_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a376aa-7b95-4052-9200-3b1fa2210c3e",
   "metadata": {},
   "source": [
    "### Comments of the results using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0e59c-7348-4a16-aa20-0b971c582b7f",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "You may have noticed that we completed our training with only ten epochs, and it took around <b>811 seconds (13.52 minutes)</b>. That means it is a reasonably long time for a small number of epochs. Imagine increasing it to 10 epochs or using a more extensive dataset like <b>CIFAR-100</b>! It would become impractical to perform this kind of task on conventional computing resources, such as a laptop with a CPU. Therefore, it is necessary to rely on much greater computational power provided by environments with GPUs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d757f4-4277-41fd-b481-12a04a2a8205",
   "metadata": {},
   "source": [
    "## Hardware and software setup used in the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7d4bb-1071-44a6-ae3c-eb067061da7c",
   "metadata": {},
   "source": [
    "The computational system used in all experiments has the operational system Red Hat Enterprise Linux, and all experiments performed in this work were run on an environment offering GPU node, where this node contains one Intel(R) i7-1165G7, CPU @4.70 GHz, 64 GB RAM, and 1 Intel Tiger Lake Gen12.  This architecture provides 16 streaming multiprocessors with 8 GB HBM2 memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f39cb-1284-4be4-82fa-25cf46aebd0d",
   "metadata": {},
   "source": [
    "##  The performance metric used in the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc6597-deb4-49c4-b457-5fdf3c7cb3c0",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "It has used the concept of <b>Speedup</b> to ensure the performance of the algorithms. In computer architecture, Speedup is a number that measures the relative performance of two systems processing the same problem. More technically, it is the improvement in the speed of execution of a task executed on two similar architectures with different resources.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1cb8f-ab67-49b8-96e5-69802ce692b1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdd67f-de47-4431-9da4-2471925c830e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style='text-align: justify;'>\n",
    "In this notebook we have shown: \n",
    "\n",
    "- The definitios about HPC applied in AI applications,\n",
    "- Some HPC uses cases, and solutions for AI using TensorFlow, and Pytorch,\n",
    "- An example of training a neural network on CPU systems using the CIFAR-10 algorithm.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4cbf28-b270-4cc3-b8dd-d6e8f9962c5e",
   "metadata": {},
   "source": [
    "## Clear the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff4e3e-b023-44b5-8b0f-91c6553bed72",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73882cd-c157-4608-83fc-07491c5a8223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8df02-d305-49f3-a5f4-8120c990b27f",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317d6bd-a969-407e-a991-cd38f5861c3e",
   "metadata": {},
   "source": [
    "In this section you learned the meaning of HPC applied in AI applications, and how we can use the processing speed of a GPU to improve the performance of our AI algorithms. In the next notebook we will study HPC as solutions for AI using the tool using TensorFlow in [_02-hpc-simulations-tensorflow.ipynb_](02-hpc-simulations-tensorflow.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
